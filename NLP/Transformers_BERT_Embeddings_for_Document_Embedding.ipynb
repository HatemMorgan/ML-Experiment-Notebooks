{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS0giQCAFPfK",
        "colab_type": "text"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpQXLwHoFMjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sentence_transformers\n",
        "# !unzip nano-1000.zip\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import xml.etree.cElementTree as et\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from google.colab import drive\n",
        "import sys\n",
        "from functools import lru_cache\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('once')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzE0yWeuTkbS",
        "colab_type": "text"
      },
      "source": [
        "# Data Preperation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRuKBXGmBtYC",
        "colab_type": "text"
      },
      "source": [
        "Only EP2081075A1 has no claims and only EP2102293A2 has no abstract. I decided not exclude them since they have another information to describe the patent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6HY7_bWKHDN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9102f6c-b1b8-4e76-ad91-0847da1f4b7a"
      },
      "source": [
        "patents = {}\n",
        "for filename in os.listdir(\"nano\"):\n",
        "  tree=et.parse(\"nano/\" + filename)\n",
        "  root=tree.getroot()\n",
        "  doc = {}\n",
        "  \n",
        "  doc[\"title\"] = root.findtext(\"Title\").strip(\"\\n\")\n",
        "  if root.findtext(\"Abstract\") is not None:\n",
        "    doc[\"abstract\"] = root.findtext(\"Abstract\").strip(\"\\n\")\n",
        "  else:\n",
        "    doc[\"abstract\"] = None\n",
        "\n",
        "  doc_claims = []\n",
        "  if root.find(\"Claims\") is not None:\n",
        "    for claim in root.find(\"Claims\").iter():\n",
        "      if len(claim.text.strip()) == 0: continue\n",
        "      doc_claims.append(claim.text.strip(\"\\n\"))\n",
        "\n",
        "  doc[\"claims\"] = doc_claims\n",
        "  patents[filename] = doc\n",
        "\n",
        "print(\"Number of patents is \", len(patents))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of patents is  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yo0_QDBUJFn",
        "colab_type": "text"
      },
      "source": [
        "# Sentence-Bert transformer models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjTjlk5UO1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PatentSentenceBert:\n",
        "  \"\"\"\n",
        "  SentenceBert: Sentence-Bert transformer model for patents data\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  model: sentence-transform\n",
        "      Model to be used to get sentence embeddings\n",
        "      \n",
        "  patents: dict\n",
        "      Dictionary contanis patents object\n",
        "\n",
        "  patent_as_multi_sent: bool\n",
        "      Indicate whether to treat a patnent as a list of sentence and use mean aggragation to get patent embedding.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, patents, patent_as_multi_sent):\n",
        "    self.model = model\n",
        "    self.patents = patents\n",
        "    self.patent_sent_func = self._patent_to_sentences if patent_as_multi_sent else self._patent_to_one_sentence\n",
        "\n",
        "  def _patent_to_one_sentence(self, patent, data):\n",
        "    \"\"\"\n",
        "    _patent_to_one_sentence function merges the patent's sentences in on giant sentence.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    patent: dict\n",
        "        Patent object.\n",
        "\n",
        "    data: List[String]\n",
        "        list of sentences to add computed sentence to it.\n",
        "\n",
        "    Returns None\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    \n",
        "    if patent[\"title\"] is not None:\n",
        "      sentences.append(patent[\"title\"])\n",
        "    if patent[\"abstract\"] is not None:\n",
        "      sentences.append(patent[\"abstract\"])\n",
        "\n",
        "    for claim in patent[\"claims\"]:\n",
        "      sentences.append(claim)\n",
        "\n",
        "    data.append(\" \".join(sentences))\n",
        "    return None\n",
        "\n",
        "  def _patent_to_sentences(self, patent, data): \n",
        "    \"\"\"\n",
        "    _patent_to_sentences function transforms a patent object into a list of sentences.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    patent: dict\n",
        "        Patent object.\n",
        "\n",
        "    data: List[String]\n",
        "        list of sentences to add computed sentences to it.\n",
        "\n",
        "    Returns None\n",
        "    \"\"\"       \n",
        "    if patent[\"title\"] is not None:\n",
        "      data.append(patent[\"title\"])\n",
        "    if patent[\"abstract\"] is not None:\n",
        "      data.append(patent[\"abstract\"])\n",
        "\n",
        "    for claim in patent[\"claims\"]:\n",
        "      data.append(claim)\n",
        "\n",
        "    return None\n",
        "\n",
        "  def get_bulk_patent_embedding(self):\n",
        "    \"\"\"\n",
        "    get_bulk_patent_embedding function computes the patent's embedding in bulk to optimize the computations\n",
        "    \n",
        "    Returns\n",
        "    ---------\n",
        "    patent_embedding: dict\n",
        "        A patent id to embedding vector mapping\n",
        "    \"\"\"\n",
        "    # collecting all sentences and keep track of range of indexes of each patent's sentences\n",
        "    patent_range = {}\n",
        "    data = []\n",
        "    start = 0\n",
        "    for p_id in self.patents.keys():\n",
        "      self.patent_sent_func(self.patents[p_id], data)\n",
        "      patent_range[p_id] = (start, len(data))\n",
        "      start = len(data)\n",
        "\n",
        "    # bulk encoding the whole sentences to sentence embedding space\n",
        "    sentence_embeddings = self.model.encode(data)\n",
        "\n",
        "    # get sentences embeddings of each patent and perform mean aggragation to get pantent embedding\n",
        "    patent_embedding = {}\n",
        "    for p_id in patent_range.keys():\n",
        "      start, end = patent_range[p_id]\n",
        "      p_sent_embedd = sentence_embeddings[start:end]\n",
        "      patent_embedding[p_id] = np.average(p_sent_embedd, axis=0)\n",
        "\n",
        "    return patent_embedding\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXTuu784vAdw",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dboCcN2M4cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_two_most_similar_patents(patent_embeddings):\n",
        "  \"\"\"\n",
        "  get_two_most_similar_patents function find the two most similar patents given a document similarity function. It\n",
        "  computes the cosine distance between each pair of patents in the embedding space\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  patent_embeddings: dict\n",
        "    dictionary mapping from patent id to its embedding vector\n",
        "  \n",
        "  Returns\n",
        "  ----------\n",
        "  res: Tuple\n",
        "      The ids of the two most similar patents and the distance between them in the embedding space.\n",
        "  \"\"\"\n",
        "  min_so_far = 1e9\n",
        "  res = None\n",
        "  keys = list(patent_embeddings.keys())\n",
        "  for i, p1 in enumerate(keys):\n",
        "    for j in range(i + 1, len(keys)):\n",
        "      p2 = keys[j]\n",
        "\n",
        "      distance = spatial.distance.cosine(patent_embeddings[p1], patent_embeddings[p2])\n",
        "      if distance < min_so_far:\n",
        "        res = (p1, p2, distance)\n",
        "        min_so_far = distance\n",
        "\n",
        "    if(i + 1) % 100 == 0:\n",
        "      print(\"Done with: \", (i+1) / 1000 * 100, '%')\n",
        "  return res"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN7yNPN2wN02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "179f467c-05ae-446a-a730-436c6965fd1b"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "bpls = PatentSentenceBert(model, patents, patent_as_multi_sent=True)\n",
        "patent_embeddings = bpls.get_bulk_patent_embedding()\n",
        "print(\"Bert transform with patent as list of sentences: \", get_two_most_similar_patents(patent_embeddings))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1464: DeprecationWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  DeprecationWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done with:  10.0 %\n",
            "Done with:  20.0 %\n",
            "Done with:  30.0 %\n",
            "Done with:  40.0 %\n",
            "Done with:  50.0 %\n",
            "Done with:  60.0 %\n",
            "Done with:  70.0 %\n",
            "Done with:  80.0 %\n",
            "Done with:  90.0 %\n",
            "Done with:  100.0 %\n",
            "Bert transform with patent as list of sentences:  ('EP2203388A1.xml', 'EP2048116A1.xml', 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUOhC6hKmC-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "81c2b86d-f166-4578-a595-9940ddbc152d"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "bpls = PatentSentenceBert(model, patents, patent_as_multi_sent=False)\n",
        "patent_embeddings = bpls.get_bulk_patent_embedding()\n",
        "print(\"Bert transform with patent as list of sentences: \", get_two_most_similar_patents(patent_embeddings))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1464: DeprecationWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  DeprecationWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done with:  10.0 %\n",
            "Done with:  20.0 %\n",
            "Done with:  30.0 %\n",
            "Done with:  40.0 %\n",
            "Done with:  50.0 %\n",
            "Done with:  60.0 %\n",
            "Done with:  70.0 %\n",
            "Done with:  80.0 %\n",
            "Done with:  90.0 %\n",
            "Done with:  100.0 %\n",
            "Bert transform with patent as list of sentences:  ('EP2203388A1.xml', 'EP2048116A1.xml', 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqZzOqflGQpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2dd8cdaa-4513-4359-9f8b-2a19b3372624"
      },
      "source": [
        "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
        "bpls = PatentSentenceBert(model, patents, patent_as_multi_sent=True)\n",
        "patent_embeddings = bpls.get_bulk_patent_embedding()\n",
        "print(\"Bert transform with patent as list of sentences: \", get_two_most_similar_patents(patent_embeddings))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 504M/504M [01:01<00:00, 8.23MB/s]\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1464: DeprecationWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  DeprecationWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done with:  10.0 %\n",
            "Done with:  20.0 %\n",
            "Done with:  30.0 %\n",
            "Done with:  40.0 %\n",
            "Done with:  50.0 %\n",
            "Done with:  60.0 %\n",
            "Done with:  70.0 %\n",
            "Done with:  80.0 %\n",
            "Done with:  90.0 %\n",
            "Done with:  100.0 %\n",
            "Bert transform with patent as list of sentences:  ('EP2057633A2.xml', 'EP2057683A2.xml', 9.417533874511719e-06)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB4Ho3sSGSOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f971a943-6f68-4fc7-8fab-f4861ce0d56c"
      },
      "source": [
        "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
        "bpls = PatentSentenceBert(model, patents, patent_as_multi_sent=False)\n",
        "patent_embeddings = bpls.get_bulk_patent_embedding()\n",
        "print(\"Bert transform with patent as list of sentences: \", get_two_most_similar_patents(patent_embeddings))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1464: DeprecationWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  DeprecationWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done with:  10.0 %\n",
            "Done with:  20.0 %\n",
            "Done with:  30.0 %\n",
            "Done with:  40.0 %\n",
            "Done with:  50.0 %\n",
            "Done with:  60.0 %\n",
            "Done with:  70.0 %\n",
            "Done with:  80.0 %\n",
            "Done with:  90.0 %\n",
            "Done with:  100.0 %\n",
            "Bert transform with patent as list of sentences:  ('EP2028662A2.xml', 'EP2031602A2.xml', 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MLTt5KesEDC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}