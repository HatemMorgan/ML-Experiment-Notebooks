{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS0giQCAFPfK",
        "colab_type": "text"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpQXLwHoFMjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import xml.etree.cElementTree as et\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from google.colab import drive\n",
        "import sys\n",
        "from functools import lru_cache\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('once')\n",
        "\n",
        "# !unzip nano-1000.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzE0yWeuTkbS",
        "colab_type": "text"
      },
      "source": [
        "# Data Preperation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRuKBXGmBtYC",
        "colab_type": "text"
      },
      "source": [
        "Only EP2081075A1 has no claims and only EP2102293A2 has no abstract. I decided not exclude them since they have another information to describe the patent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6HY7_bWKHDN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e62eb9e6-3222-4b1e-e384-24697e206038"
      },
      "source": [
        "patents = {}\n",
        "for filename in os.listdir(\"nano\"):\n",
        "  tree=et.parse(\"nano/\" + filename)\n",
        "  root=tree.getroot()\n",
        "  doc = {}\n",
        "  \n",
        "  doc[\"title\"] = root.findtext(\"Title\")\n",
        "  doc[\"abstract\"] = root.findtext(\"Abstract\")\n",
        "\n",
        "  doc_claims = []\n",
        "  if root.find(\"Claims\") is not None:\n",
        "    for claim in root.find(\"Claims\").iter():\n",
        "      if len(claim.text.strip()) == 0: continue\n",
        "      doc_claims.append(claim.text)\n",
        "\n",
        "  doc[\"claims\"] = doc_claims\n",
        "  patents[filename] = doc\n",
        "\n",
        "print(\"Number of patents is \", len(patents))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of patents is  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yo0_QDBUJFn",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB_TwdORUR7F",
        "colab_type": "text"
      },
      "source": [
        "### Model Data Preperation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBiu8VNVUWj",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec model expects data as a list of lists of words. I am doing a mild pre-processing using gensim.utils.simple_preprocess (line). This does some basic pre-processing such as tokenization, lowercasing, removing single-letter words (e.g. \"a\"), etc. and returns back a list of tokens (words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjTjlk5UO1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5403d067-54cd-4567-a828-514b9199c2c4"
      },
      "source": [
        "data = []\n",
        "for p in patents:\n",
        "  if patents[p][\"title\"] is not None:\n",
        "    data.append(gensim.utils.simple_preprocess(patents[p][\"title\"], min_len=2, max_len=30))\n",
        "  if patents[p][\"abstract\"] is not None:\n",
        "    data.append(gensim.utils.simple_preprocess(patents[p][\"abstract\"], min_len=2, max_len=30))\n",
        "    \n",
        "  for claim in patents[p][\"claims\"]:\n",
        "    data.append(gensim.utils.simple_preprocess(claim, min_len=2, max_len=30))\n",
        "\n",
        "print('Number of sentences = ', len(data))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences =  37060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHuUjS74XXJt",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYTGYrbxXav9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_model = gensim.models.Word2Vec(data, size=100, min_count=5, window=5, sg=1) # skip-gram model\n",
        "word2vec_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec (L2-normalized vectors.)."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcL66bRzfSC1",
        "colab_type": "text"
      },
      "source": [
        "#  Similarity Measurement\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQRYiFI_vD-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@lru_cache(maxsize=1000)\n",
        "def patent_to_doc(patent_id):\n",
        "  \"\"\"\n",
        "  patent_to_doc function transforms a patent object into a list of sentences. It uses LRU cache for memoization to\n",
        "  speed up computations.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  patent_id: str\n",
        "      The ID of the patent to extract it from patents dictionary\n",
        "  \n",
        "  Returns\n",
        "  ---------\n",
        "  sentences: List[str]\n",
        "      list of sentences.\n",
        "  \"\"\"\n",
        "  sentences = []\n",
        "  if patents[patent_id][\"title\"] is not None:\n",
        "    sentences.append(patents[patent_id][\"title\"])\n",
        "\n",
        "  if patents[patent_id][\"abstract\"] is not None:\n",
        "    sentences.append(patents[patent_id][\"abstract\"])\n",
        "\n",
        "  for claim in patents[p][\"claims\"]:\n",
        "    sentences.append(claim)\n",
        "\n",
        "  return sentences\n",
        "\n",
        "@lru_cache(maxsize=1000)\n",
        "def get_doc_embedding(p_id, model):\n",
        "  \"\"\"\n",
        "  get_doc_embedding function computes document embedding by averaging all its word embeddings. It uses LRU cache\n",
        "  for memoization to speed up computations.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  patent_id: str\n",
        "      The ID of the patent to extract it from patents dictionary\n",
        "  Returns\n",
        "  ----------\n",
        "  doc_embedding: np.array\n",
        "      document embedding vector.\n",
        "  \"\"\"\n",
        "  doc = patent_to_doc(p_id)   \n",
        "  doc_embedd = []\n",
        "  for sent in doc:\n",
        "      tokens = gensim.utils.simple_preprocess(sent, min_len=2, max_len=30) # mild pre-processing\n",
        "      sent_embedd = [model[w] for w in tokens if w in model] # remove words that is not in model vocabulary\n",
        "      doc_embedd.append(np.average(sent_embedd, axis=0)) # average word embedding to get sentence embedding\n",
        "\n",
        "  return np.average(doc_embedd, axis=0) # average sentence embedding to get document embedding\n",
        "\n",
        "def avg_cos_similarity(patent1_id, patent2_id, model):\n",
        "    \"\"\"\n",
        "    avg_cos_similarity function used to find the cosin similarity between two documents by using average aggregator over\n",
        "    document's word embeddings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    patent1_id: str\n",
        "        The ID of the first patent.\n",
        "    \n",
        "    patent2_id: str\n",
        "        The ID of the second patent.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    distance: float\n",
        "       The Cosine distance.\n",
        "    \"\"\"\n",
        "\n",
        "    doc1_embedd = get_doc_embedding(patent1_id, model)\n",
        "    doc2_embedd = get_doc_embedding(patent2_id, model)\n",
        "    distance = spatial.distance.cosine(doc1_embedd, doc2_embedd)\n",
        "    return distance \n",
        "\n",
        "  \n",
        "@lru_cache(maxsize=1000)\n",
        "def get_doc_tokens(p_id):\n",
        "  \"\"\"\n",
        "  get_doc_tokens function merges the patent's sentences in on giant sentence. It uses LRU cache\n",
        "  for memoization to speed up computations.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  p_id: str \n",
        "      The ID of the patent to extract it from patents dictionary.\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tokens: str\n",
        "      patent as a single sentence.\n",
        "  \"\"\"\n",
        "\n",
        "  doc = patent_to_doc(p_id)\n",
        "  text = \". \".join([sent for sent in doc])\n",
        "  tokens = gensim.utils.simple_preprocess(text, min_len=2, max_len=30)  \n",
        "  return tokens\n",
        "\n",
        "def wmd_similarity(patent1_id, patent2_id, model):\n",
        "  \"\"\"\n",
        "  wmd_similarity function used to find the Word Mover's Distance which uses the word embeddings of the words in two\n",
        "  texts to measure the minimum amount that the words in one text need to travel in semantic space to reach the words\n",
        "  of the other text. (gives better results than 1). We treated the document as bag of words.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  patent1_id: str \n",
        "      The ID of the first patent\n",
        "\n",
        "  patent2_id: str\n",
        "      The ID of the second patent\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  similarity: float\n",
        "      Word Mover's Distance, the smaller the distance the closer the docuements in the embeddings space\n",
        "  \"\"\"\n",
        "  tokens1 = get_doc_tokens(patent1_id)\n",
        "  tokens2 = get_doc_tokens(patent2_id)\n",
        "\n",
        "  similarity = model.wv.wmdistance(tokens1, tokens2)\n",
        "  return similarity\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXTuu784vAdw",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dboCcN2M4cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "8a0a441a-cad8-4555-e566-1134599bd31c"
      },
      "source": [
        "def get_two_most_similar_patents(doc_similarity_func):\n",
        "  \"\"\"\n",
        "  get_two_most_similar_patents function find the two most similar patents given a document similarity function. It\n",
        "  computes the similarity between each pair of patents and find most similar ones\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  doc_similarity_func: function: (patent1_id:(Stirng), patent2_id: (String), model:(gensim.models))-> float)\n",
        "    function used to compute distance between patent's in the embedding space. \n",
        "  \n",
        "  Returns\n",
        "  ----------\n",
        "  res: Tuple\n",
        "      The ids of the two most similar patents and the distance between them in the embedding space.\n",
        "  \"\"\"\n",
        "\n",
        "  min_so_far = 1e9\n",
        "  res = None\n",
        "  keys = list(patents.keys())\n",
        "  for i, p1 in enumerate(keys):\n",
        "    for j in range(i + 1, len(keys)):\n",
        "      p2 = keys[j]\n",
        "      distance = doc_similarity_func(p1, p2, word2vec_model)\n",
        "      if distance < min_so_far:\n",
        "        res = (p1, p2, distance)\n",
        "        min_so_far = distance\n",
        "\n",
        "    if(i + 1) % 100 == 0:\n",
        "      print(\"Done with: \", (i+1) / 1000 * 100, '%')\n",
        "  return res\n",
        "\n",
        "print(\"With cosine distance: \", get_two_most_similar_patents(avg_cos_similarity))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done with:  10.0 %\n",
            "Done with:  20.0 %\n",
            "Done with:  30.0 %\n",
            "Done with:  40.0 %\n",
            "Done with:  50.0 %\n",
            "Done with:  60.0 %\n",
            "Done with:  70.0 %\n",
            "Done with:  80.0 %\n",
            "Done with:  90.0 %\n",
            "Done with:  100.0 %\n",
            "With cosine distance:  ('EP1973178A2.xml', 'EP2166581A2.xml', 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuoks3VvzYdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"With Word Mover's Distance similarity: \", get_similar_patents(wmd_similarity))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUOhC6hKmC-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f89b38ac-202b-4caa-af51-ca19a06782ea"
      },
      "source": [
        "print(patent_to_doc.cache_info())\n",
        "print(get_doc_embedding.cache_info())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CacheInfo(hits=0, misses=1000, maxsize=1000, currsize=1000)\n",
            "CacheInfo(hits=998000, misses=1000, maxsize=1000, currsize=1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MLTt5KesEDC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}